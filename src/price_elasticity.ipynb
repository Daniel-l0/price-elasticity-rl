{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_id        0\n",
      "product_id     0\n",
      "category_id    0\n",
      "sales          0\n",
      "price          0\n",
      "units          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Pre-procesamiento: Revisamos el dataframe y validamos su calidad\n",
    "\n",
    "df = pd.read_csv('../data/processed/masked_data.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "null_values = df.isnull().sum()\n",
    "print(null_values)\n",
    "\n",
    "df['date_id'] = pd.to_datetime(df['date_id'])\n",
    "df.sort_values(by=['product_id', 'date_id'], inplace=True)\n",
    "df['month'] = df['date_id'].dt.month\n",
    "df['day_of_week'] = df['date_id'].dt.dayofweek # fin de semana es 5 y 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno (PricingEnv):\n",
    "\n",
    "* Recibe el dataframe con price, sales y units.\n",
    "* Define las acciones: aumentar, disminuir o mantener el precio.\n",
    "* Calcula la demanda usando un modelo de elasticidad-precio con un coeficiente de -1.5.\n",
    "* Finaliza el episodio cuando se acaban los datos o el inventario llega a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entorno de gym para el problema de pricing\n",
    "\n",
    "\n",
    "class PricingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(PricingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(df)\n",
    "        self.action_space = spaces.Discrete(3)  # 0: bajar precio, 1: mantener, 2: subir precio\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):  \n",
    "        self.current_step = 0\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        self.state = np.array([row['price'], row['sales']])\n",
    "        return self.state\n",
    "\n",
    "    def estimate_demand(self, price, prev_price, prev_sales): # Modelo de elasticidad de precio para estimar demanda\n",
    "        price_elasticity = 1  # Ajuste elastico de precio\n",
    "        return prev_sales * (price / prev_price) ** price_elasticity\n",
    "\n",
    "    def step(self, action):\n",
    "        price, sales = self.state\n",
    "        new_price = price * (0.97 if action == 0 else 1.03 if action == 2 else 1.0)\n",
    "        new_sales = self.estimate_demand(new_price, price, sales)\n",
    "        revenue = new_price * new_sales\n",
    "        \n",
    "        self.state = np.array([new_price, new_sales])\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        return self.state, revenue, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step}, Price: {self.state[0]}, Sales: {self.state[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente (QLearningAgent):\n",
    "\n",
    "* Usa una tabla Q para almacenar valores de estado-acción.\n",
    "* Implementa una política epsilon-greedy para la exploración-explotación.\n",
    "* Actualiza los valores Q con la ecuación de Bellman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente de Q-Learning para el problema de pricing\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_space, action_space, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.q_table = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        return self.q_table.setdefault(tuple(state), np.zeros(self.action_space))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        return np.argmax(self.get_q_values(state))\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        q_values = self.get_q_values(state)\n",
    "        next_q_values = self.get_q_values(next_state)\n",
    "        q_values[action] += self.alpha * (reward + self.gamma * np.max(next_q_values) - q_values[action])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento:\n",
    "\n",
    "* Se ejecutan múltiples episodios para aprender la mejor política de precios.\n",
    "\n",
    "## Optimización de hiperparámetros:\n",
    "\n",
    "* Usa una búsqueda en cuadrícula (GridSearch) probando diferentes valores de alpha, gamma y epsilon.\n",
    "Evalúa el rendimiento midiendo la recompensa promedio en varias simulaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del agente de Q-Learning\n",
    "\n",
    "def train_q_learning(env, agent, episodes=500):\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    return agent\n",
    "\n",
    "\n",
    "# Optimización de hiperparámetros\n",
    "\n",
    "def optimize_hyperparams(df):\n",
    "    env = PricingEnv(df)\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 0.5, 0.9],\n",
    "        'gamma': [0.7, 0.9, 0.99],\n",
    "        'epsilon': [0.1, 0.2, 0.3]\n",
    "    }\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    \n",
    "    for alpha in param_grid['alpha']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            for epsilon in param_grid['epsilon']:\n",
    "                agent = QLearningAgent(env.observation_space.shape[0], env.action_space.n, alpha, gamma, epsilon)\n",
    "                trained_agent = train_q_learning(env, agent)\n",
    "                score = np.mean([sum(env.step(trained_agent.choose_action(env.reset()))[1] for _ in range(10))])\n",
    "                if score > best_score:\n",
    "                    best_score, best_params = score, {'alpha': alpha, 'gamma': gamma, 'epsilon': epsilon}\n",
    "    \n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = pd.read_csv('../data/processed/masked_data.csv')  # Cargar datos\n",
    "    df['date_id'] = pd.to_datetime(df['date_id'])\n",
    "    df.sort_values(by=['product_id', 'date_id'], inplace=True)\n",
    "    df['month'] = df['date_id'].dt.month\n",
    "    df['day_of_week'] = df['date_id'].dt.dayofweek\n",
    "    \n",
    "    env = PricingEnv(df)\n",
    "    best_params = optimize_hyperparams(df)\n",
    "    \n",
    "    agent = QLearningAgent(env.observation_space.shape[0], env.action_space.n, \n",
    "                           best_params['alpha'], best_params['gamma'], best_params['epsilon'])\n",
    "    trained_agent = train_q_learning(env, agent)\n",
    "    \n",
    "    # Evaluación del modelo\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = trained_agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        state = next_state\n",
    "    \n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
